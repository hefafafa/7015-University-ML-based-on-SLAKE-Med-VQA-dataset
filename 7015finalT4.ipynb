{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Step 1: Environment Setup\n",
        "# ==========================================\n",
        "!pip install transformers datasets accelerate torch torchvision clean-text matplotlib seaborn\n",
        "\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import torch\n",
        "import random\n",
        "import string\n",
        "import collections\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
        "from tqdm.auto import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# Force clear GPU memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Parameter Configuration ---\n",
        "ZIP_PATH = '/content/drive/MyDrive/MV_Project/Slake.zip'\n",
        "EXTRACT_PATH = '/content/slake_final_viz_best'\n",
        "CHECKPOINT_PATH = '/content/drive/MyDrive/MV_Project/blip_final_best_v8.pth'\n",
        "\n",
        "# --- Core Decision: Revert to 4x8 for maximum stability ---\n",
        "BATCH_SIZE = 4            # Physical Batch (Prevent VRAM OOM)\n",
        "GRAD_ACCUM_STEPS = 8      # [Revert to 8] Virtual Batch = 32 (Gold Standard)\n",
        "LEARNING_RATE = 2e-5      # Fine-tuning learning rate\n",
        "NUM_EPOCHS = 15\n",
        "PATIENCE = 5\n",
        "BASELINE_ACC = 73.8\n",
        "\n",
        "print(\"[INFO] Environment setup completed.\")"
      ],
      "metadata": {
        "id": "wVF2Jejo_Far"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Step 2: Data Visualization\n",
        "# ==========================================\n",
        "if not os.path.exists(EXTRACT_PATH):\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(EXTRACT_PATH)\n",
        "\n",
        "img_dir = None\n",
        "json_path = None\n",
        "for root, dirs, files in os.walk(EXTRACT_PATH):\n",
        "    if '__MACOSX' in root: continue\n",
        "    if 'imgs' in dirs: img_dir = os.path.join(root, 'imgs')\n",
        "    elif 'images' in dirs: img_dir = os.path.join(root, 'images')\n",
        "    for file in files:\n",
        "        if file == 'train.json': json_path = os.path.join(root, file)\n",
        "\n",
        "print(f\"[INFO] JSON Path: {json_path}\")\n",
        "\n",
        "def visualize_dataset(json_file):\n",
        "    print(\"[INFO] Generating Data Plots...\")\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    en_data = [x for x in data if x.get('q_lang') == 'en']\n",
        "\n",
        "    q_types = []\n",
        "    answers = []\n",
        "    for item in en_data:\n",
        "        ans = str(item['answer']).lower().strip()\n",
        "        answers.append(ans)\n",
        "        if ans in ['yes', 'no']:\n",
        "            q_types.append('Closed (Yes/No)')\n",
        "        else:\n",
        "            q_types.append('Open-Ended')\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\")\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    type_counts = collections.Counter(q_types)\n",
        "    plt.pie(type_counts.values(), labels=type_counts.keys(), autopct='%1.1f%%',\n",
        "            colors=['#66b3ff','#ff9999'], explode=(0.05, 0))\n",
        "    plt.title('Question Type Distribution')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    ans_counts = collections.Counter(answers).most_common(10)\n",
        "    sns.barplot(x=[x[1] for x in ans_counts], y=[x[0] for x in ans_counts], palette='viridis')\n",
        "    plt.title('Top 10 Answers')\n",
        "    plt.xlabel('Count')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_dataset(json_path)"
      ],
      "metadata": {
        "id": "Sc9dmvrF_yBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Extra Feature: Visualize Samples\n",
        "# ==========================================\n",
        "\n",
        "def show_data_samples(json_file, img_dir, num_samples=3):\n",
        "    print(f\"[INFO] Displaying {num_samples} Random Samples from Dataset...\")\n",
        "\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    # Filter only English data\n",
        "    en_data = [x for x in data if x.get('q_lang') == 'en']\n",
        "\n",
        "    # Random sampling\n",
        "    samples = random.sample(en_data, num_samples)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    for i, item in enumerate(samples):\n",
        "        img_path = os.path.join(img_dir, item['img_name'])\n",
        "        question = item['question']\n",
        "        answer = item['answer']\n",
        "        q_type = \"Closed\" if str(answer).lower() in ['yes', 'no'] else \"Open\"\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            plt.subplot(1, num_samples, i+1)\n",
        "            plt.imshow(image)\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Display Q&A above the image\n",
        "            # Note: textwrap helps prevent questions from exceeding image width (logic not strictly implemented here but recommended)\n",
        "            title_text = f\"Q: {question}\\nA: {answer} ({q_type})\"\n",
        "            plt.title(title_text, fontsize=12, color='blue', loc='left')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Cannot load image {img_path}: {e}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run visualization\n",
        "show_data_samples(json_path, img_dir, num_samples=4)"
      ],
      "metadata": {
        "id": "8NKPK8ohQxsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Step 3: Dataset & Metrics\n",
        "# ==========================================\n",
        "def normalize_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text.strip()\n",
        "\n",
        "def compute_f1_score(pred, truth):\n",
        "    pred_tokens = normalize_text(pred).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "    common = collections.Counter(pred_tokens) & collections.Counter(truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0: return 0\n",
        "    precision = 1.0 * num_same / len(pred_tokens)\n",
        "    recall = 1.0 * num_same / len(truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def compute_accuracy_robust(pred, truth, q_type):\n",
        "    p_norm = normalize_text(pred)\n",
        "    t_norm = normalize_text(truth)\n",
        "    if q_type == 'closed':\n",
        "        if t_norm == 'yes' and 'yes' in p_norm: return 1\n",
        "        if t_norm == 'no' and 'no' in p_norm: return 1\n",
        "        return 0\n",
        "    if t_norm in p_norm: return 1\n",
        "    if p_norm in t_norm and len(p_norm) > 2: return 1\n",
        "    return 0\n",
        "\n",
        "class SlakeDataset(Dataset):\n",
        "    def __init__(self, json_path, img_dir, processor):\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            self.data = json.load(f)\n",
        "        self.en_data = [x for x in self.data if x.get('q_lang') == 'en']\n",
        "        self.img_dir = img_dir\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.en_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.en_data[idx]\n",
        "        img_path = os.path.join(self.img_dir, item['img_name'])\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except:\n",
        "            image = Image.new('RGB', (224, 224))\n",
        "\n",
        "        question = item['question']\n",
        "        raw_answer = item['answer']\n",
        "        answer = str(raw_answer).lower()\n",
        "\n",
        "        text_input = f\"Question: {question} Answer:\"\n",
        "        q_type = \"closed\" if normalize_text(answer) in ['yes', 'no'] else \"open\"\n",
        "\n",
        "        encoding = self.processor(image, text_input, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        labels = self.processor.tokenizer(\n",
        "            answer, max_length=8, padding=\"max_length\", truncation=True, return_tensors='pt'\n",
        "        ).input_ids\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": encoding.pixel_values.squeeze(),\n",
        "            \"input_ids\": encoding.input_ids.squeeze(),\n",
        "            \"labels\": labels.squeeze(),\n",
        "            \"answer_text\": raw_answer,\n",
        "            \"q_type\": q_type\n",
        "        }\n",
        "print(\"[INFO] Dataset metrics ready.\")"
      ],
      "metadata": {
        "id": "UxkeiyRQ_33l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Step 4: Model Loading (Model & Freeze)\n",
        "# ==========================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[INFO] Loading BLIP on {device}...\")\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(device)\n",
        "\n",
        "# --- Freeze Vision Layers ---\n",
        "for param in model.vision_model.parameters():\n",
        "    param.requires_grad = False\n",
        "print(\"[INFO] Vision Encoder frozen.\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
        "\n",
        "full_dataset = SlakeDataset(json_path, img_dir, processor)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "train_set, test_set = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"[INFO] Train: {len(train_set)}, Test: {len(test_set)}\")"
      ],
      "metadata": {
        "id": "aEDu740U_76s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Step 5: Training Loop - 4x8\n",
        "# ==========================================\n",
        "def evaluate_metrics(loader):\n",
        "    model.eval()\n",
        "    t_acc, t_f1, open_c, open_t, closed_c, closed_t, t_loss, cnt = 0, 0, 0, 0, 0, 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Eval\", leave=False):\n",
        "            p_v = batch['pixel_values'].to(device)\n",
        "            i_i = batch['input_ids'].to(device)\n",
        "            lbl = batch['labels'].to(device)\n",
        "            ans = batch['answer_text']\n",
        "            typ = batch['q_type']\n",
        "\n",
        "            out = model(input_ids=i_i, pixel_values=p_v, labels=lbl)\n",
        "            t_loss += out.loss.item() * len(ans)\n",
        "\n",
        "            gen = model.generate(pixel_values=p_v, input_ids=i_i, max_new_tokens=10)\n",
        "            preds = processor.batch_decode(gen, skip_special_tokens=True)\n",
        "\n",
        "            for p, t, qt in zip(preds, ans, typ):\n",
        "                acc = compute_accuracy_robust(p, t, qt)\n",
        "                f1 = compute_f1_score(p, t)\n",
        "                t_acc += acc; t_f1 += f1\n",
        "                if qt == 'closed': closed_c += acc; closed_t += 1\n",
        "                else: open_c += acc; open_t += 1\n",
        "            cnt += len(ans)\n",
        "\n",
        "    return (t_loss/cnt, t_acc/cnt*100, t_f1/cnt,\n",
        "            open_c/open_t*100 if open_t>0 else 0,\n",
        "            closed_c/closed_t*100 if closed_t>0 else 0)\n",
        "\n",
        "history = {k: [] for k in ['tr_loss','te_loss','te_acc','te_f1','te_o_acc','te_c_acc']}\n",
        "best_acc = 0.0\n",
        "patience_cnt = 0\n",
        "\n",
        "print(f\"[INFO] Start Training (Virtual Batch = {BATCH_SIZE * GRAD_ACCUM_STEPS})...\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "    tr_loss_acc = 0\n",
        "    steps = 0\n",
        "\n",
        "    for idx, batch in enumerate(loop):\n",
        "        p_v = batch['pixel_values'].to(device)\n",
        "        i_i = batch['input_ids'].to(device)\n",
        "        lbl = batch['labels'].to(device)\n",
        "\n",
        "        out = model(input_ids=i_i, pixel_values=p_v, labels=lbl)\n",
        "        loss = out.loss / GRAD_ACCUM_STEPS\n",
        "        loss.backward()\n",
        "        tr_loss_acc += loss.item() * GRAD_ACCUM_STEPS\n",
        "        steps += 1\n",
        "\n",
        "        if (idx + 1) % GRAD_ACCUM_STEPS == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            loop.set_postfix(loss=loss.item() * GRAD_ACCUM_STEPS)\n",
        "\n",
        "    tr_loss = tr_loss_acc / steps\n",
        "    te_loss, te_acc, te_f1, te_o, te_c = evaluate_metrics(test_loader)\n",
        "\n",
        "    history['tr_loss'].append(tr_loss); history['te_loss'].append(te_loss)\n",
        "    history['te_acc'].append(te_acc); history['te_f1'].append(te_f1)\n",
        "    history['te_o_acc'].append(te_o); history['te_c_acc'].append(te_c)\n",
        "\n",
        "    print(f\"\\n[Epoch {epoch+1}] Acc: {te_acc:.2f}% (Best: {best_acc:.2f}%) | F1: {te_f1:.4f}\")\n",
        "    print(f\"Open: {te_o:.2f}% | Closed: {te_c:.2f}% | Loss: {te_loss:.4f}\")\n",
        "\n",
        "    if te_acc > best_acc:\n",
        "        best_acc = te_acc\n",
        "        patience_cnt = 0\n",
        "        torch.save(model.state_dict(), CHECKPOINT_PATH)\n",
        "        print(\">>> New Best Model Saved!\")\n",
        "    else:\n",
        "        patience_cnt += 1\n",
        "        if patience_cnt >= PATIENCE:\n",
        "            print(\">>> Early Stopping!\")\n",
        "            break"
      ],
      "metadata": {
        "id": "fIfwbhAH__0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Step 6: Final Visualization & Report (Viz & Report)\n",
        "# ==========================================\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# --- Part 1: Generate 9 Plots (3x3 Grid) ---\n",
        "print(\"[INFO] Generating 3x3 Analytical Plots...\")\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
        "epochs = range(1, len(history['total_acc_te']) + 1)\n",
        "\n",
        "# Helper Plotting Function\n",
        "def plot_ax(ax, tr_data, te_data, title, ylabel, baseline=None, color_te='tab:blue'):\n",
        "    ax.plot(epochs, tr_data, label='Train', marker='.', linestyle='--', color='gray', alpha=0.6)\n",
        "    ax.plot(epochs, te_data, label='Test (Ours)', marker='o', linewidth=2, color=color_te)\n",
        "    if baseline:\n",
        "        ax.axhline(y=baseline, color='tab:red', linestyle='--', linewidth=2, label=f'Baseline ({baseline}%)')\n",
        "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
        "    ax.set_xlabel('Epochs')\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.legend(loc='best')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# --- Row 1: Loss ---\n",
        "plot_ax(axes[0,0], history['total_loss_tr'], history['total_loss_te'], 'Total Loss Convergence', 'Loss', color_te='tab:blue')\n",
        "plot_ax(axes[0,1], history['open_loss_tr'], history['open_loss_te'], 'Open-Ended Loss', 'Loss', color_te='tab:green')\n",
        "plot_ax(axes[0,2], history['closed_loss_tr'], history['closed_loss_te'], 'Closed-Ended Loss', 'Loss', color_te='tab:orange')\n",
        "\n",
        "# --- Row 2: Accuracy ---\n",
        "plot_ax(axes[1,0], history['total_acc_tr'], history['total_acc_te'], 'Total Accuracy vs Baseline', 'Accuracy (%)', baseline=BASELINE_ACC, color_te='tab:blue')\n",
        "plot_ax(axes[1,1], history['open_acc_tr'], history['open_acc_te'], 'Open-Ended Accuracy', 'Accuracy (%)', color_te='tab:green')\n",
        "plot_ax(axes[1,2], history['closed_acc_tr'], history['closed_acc_te'], 'Closed-Ended Accuracy', 'Accuracy (%)', color_te='tab:orange')\n",
        "\n",
        "# --- Row 3: F1 Score (Semantic Understanding) ---\n",
        "plot_ax(axes[2,0], history['total_f1_tr'], history['total_f1_te'], 'Total F1 Score', 'F1 Score', color_te='tab:purple')\n",
        "plot_ax(axes[2,1], history['open_f1_tr'], history['open_f1_te'], 'Open-Ended F1', 'F1 Score', color_te='tab:purple')\n",
        "plot_ax(axes[2,2], history['closed_f1_tr'], history['closed_f1_te'], 'Closed-Ended F1', 'F1 Score', color_te='tab:purple')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Part 2: Generate Text Report ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"             FINAL EXPERIMENTAL REPORT: MED-VQA (BLIP OPTIMIZED)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Experimental Setup Overview\n",
        "print(f\"1. EXPERIMENTAL SETUP\")\n",
        "print(f\"   - Model Architecture    : BLIP (Salesforce/blip-vqa-base)\")\n",
        "print(f\"   - Vision Encoder        : Frozen (Pre-trained weights locked)\")\n",
        "print(f\"   - Optimization Strategy : Gradient Accumulation (Steps={GRAD_ACCUM_STEPS})\")\n",
        "print(f\"   - Effective Batch Size  : {BATCH_SIZE * GRAD_ACCUM_STEPS} (Physical: {BATCH_SIZE})\")\n",
        "print(f\"   - Total Epochs Trained  : {len(history['total_acc_te'])}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# 2. Final Core Metrics\n",
        "final_acc = best_acc\n",
        "final_f1 = history['total_f1_te'][-1]\n",
        "final_open = history['open_acc_te'][-1]\n",
        "final_closed = history['closed_acc_te'][-1]\n",
        "\n",
        "print(f\"2. PERFORMANCE METRICS (TEST SET)\")\n",
        "print(f\"   ------------------------------------------------------------\")\n",
        "print(f\"   | Metric                | Value       | vs. Baseline ({BASELINE_ACC}%) |\")\n",
        "print(f\"   ------------------------------------------------------------\")\n",
        "print(f\"   | Total Accuracy        | {final_acc:.2f}%      | {'+' if final_acc >= BASELINE_ACC else ''}{final_acc - BASELINE_ACC:.2f}%           |\")\n",
        "print(f\"   | F1 Score              | {final_f1:.4f}      | N/A                 |\")\n",
        "print(f\"   | Open-Ended Accuracy   | {final_open:.2f}%      | (Critical Metric)   |\")\n",
        "print(f\"   | Closed-Ended Accuracy | {final_closed:.2f}%      | -                   |\")\n",
        "print(f\"   ------------------------------------------------------------\")\n",
        "\n",
        "# 3. Automated Conclusion (for Report Conclusion)\n",
        "print(\"-\" * 80)\n",
        "print(f\"3. AUTOMATED CONCLUSION\")\n",
        "if final_acc > BASELINE_ACC:\n",
        "    print(f\"   SUCCESS: The optimized BLIP model has successfully outperformed the CNN-LSTM\")\n",
        "    print(f\"   baseline by a margin of {final_acc - BASELINE_ACC:.2f}%.\")\n",
        "    print(f\"   Key Observation: The high F1 score ({final_f1:.4f}) indicates that the generative\")\n",
        "    print(f\"   model effectively understands medical semantics beyond simple classification.\")\n",
        "else:\n",
        "    print(f\"   RESULT: The model achieved comparable performance to the baseline.\")\n",
        "    print(f\"   Further optimization (e.g., more data, longer training) may be required.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "nevlLhpKAJbS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}